image_size: 256
experiment_name: unconditional

optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  weight_decay: 0


vae:
  _target_: diffusers.models.AutoencoderKL.from_pretrained
  pretrained_model_name_or_path: "transformers/sd-vae-ft-ema"


data:
  dataset:
    _target_: speed.dataset.image.image_dataset
    image_size: ${image_size}
    class_cond: false
    root: "root_path_dataset"

  batch_size: 32
  num_workers: 4

model:
  _target_: speed.networks.dit.mdt.MDTv2_XL_2
  condition: none
  mask_ratio: 0.3


sample:
  diffusion:
    timestep_respacing: '250'

inference:
  diffusion:
    timestep_respacing: '250'
  per_proc_batch_size: 16
  num_samples: 10000

epoch: 100_000
max_training_steps: 100_000

log_every: 100
ckpt_every: 10_000
